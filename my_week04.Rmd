---
title: "Homework 4"
output:
  pdf_document: default
---

```{r setup, include=FALSE}
library(rethinking)
library(ggplot2)
library(dplyr)
library(tidyr)
library(brms)
library(here)
```

### Question 7E1

State the three motivating criteria that define information entropy. Try to express each in your
own words

1 - information entropy should be countinous

2 - the mesure of uncertinity should increase as the number of the possible events increases

3 - uncertinity should be additive


### Question 7E2

Suppose a coin is weighted such that, when it is tossed and lands on a table, it comes up heads
70% of the time. What is the entropy of this coin?


$H(p) = - E log (p_i)$
$H(p) = - 0.3 \times log(0.3) + 0.7 \times log(0.7)$

```{r}
- 0.3 * log(0.3) + 0.7 * log(0.7)
```

### Question 7E3

Suppose a four-sided die is loaded such that, when tossed onto a table, it shows “1” 20%, “2”
25%, ”3” 25%, and ”4” 30% of the time. What is the entropy of this die?

$H(p) = - E log (p_i)$
$H(p) = - 0.2 \times log(0.2) + 0.25 \times log(0.25) + 0.25 \times log(0.25) + 0.30 \times log(0.30)$


```{r}
- (0.2 * log(0.2) + 0.25 * log(0.25) + 0.25 * log(0.25) + 0.30 * log(0.30))
```
### Question 7E4


Suppose another four-sided die is loaded such that it never shows “4”. The other three sides
show equally often. What is the entropy of this die?


$H(p) = - E log (p_i)$
$H(p) = - 1/3 \times log(1/3) + 1/3 \times log(1/3) + 1/3 \times log(1/3) + 1/3 \times log(1/3)$

```{r}
- (1/3 * log(1/3) + 1/3 * log(1/3) + 1/3 * log(1/3))

```

### Question 7M3


When comparing models with an information criterion, why must all models be fit to exactly
the same observations? What would happen to the information criterion values, if the models were
fit to different numbers of observations? Perform some experiments, if you are not sure.


```{r}

set.seed(2020)


number_of_observations <- rep(c(100, 500, 1000), each = 100)

sample_data <- purrr::map(number_of_observations, function (n) {
  tibble(x1 = rnorm(n=n)) %>%
    mutate(y = rnorm(n=n, mean=0.8*x1)) %>%
    mutate(across(everything(), standardize))
})


models <- purrr::map(sample_data,
  function(df) {
    mod <- quap(alist(y ~ dnorm(mu, sigma),
                      mu <- alpha + beta * x1,
                      alpha ~ dnorm(0, 0.2),
                      beta ~ dnorm(0, 0.5),
                      sigma ~ dexp(1)),
                data = df, start = list(alpha = 0, beta = 0))
    return(mod)
  })

lppd <- purrr::map(models, function(x) { sum(rethinking::lppd(x))  })

plot(unlist(lppd))

```


### Question 7M4


What happens to the effective number of parameters, as measured by PSIS or WAIC, as a prior
becomes more concentrated? Why? Perform some experiments, if you are not sure.


```{r}
set.seed(2020)

N <- 20

sample_data <- purrr::map(rep(0, 300), function(x) { 
  tibble(x1 = rnorm(n=N), x2 = rnorm(n=N), x3 = rnorm(n=N)) %>% 
    mutate(y = rnorm(n = N, mean = 0.3 + 0.8 * x1 + 0.6 * x2 + 1.2 * x3))
})

priors <- rep(c(0.1, 1, 10))

models <- purrr::map(priors, function (p_sd) { 
  brm(y ~ 1 + x1 + x2 + x3 , data = sample_data[[1]],
   family = gaussian,
   prior = c(prior(normal(0, 0.2), class = Intercept),
              prior_string(paste0("normal(0,", p_sd, ")"), class = "b"),
             prior(exponential(1), class = sigma)
  ))
  }) %>% rep(each=100)



updated_models <- purrr::map2(sample_data, models, function (df, model) update(model, newdata = df))

x1 <- purrr::map(updated_models, function (model) { return(fixef(model)['x1', 'Estimate'])}) %>% unlist()
x2 <- purrr::map(updated_models, function (model) { return(fixef(model)['x2', 'Estimate'])}) %>% unlist()
x3 <- purrr::map(updated_models, function (model) { return(fixef(model)['x3', 'Estimate'])}) %>% unlist()

waic_values <- purrr::map(updated_models, function (model) { return(loo::waic(model)$estimates["p_waic", "Estimate"])})
loo_values <- purrr::map(updated_models, function (model) { return(loo::loo(model)$estimates["p_loo", "Estimate"])})


##


```


```{r}

prior_factor <- rep(as.factor(priors), each=100)

df <- rbind(data.frame(prior=prior_factor, type='waic', value=waic_values %>% unlist),
            data.frame(prior=prior_factor, type='loo', value=loo_values %>% unlist))

ggplot(df, aes(x=value)) + 
  facet_grid(rows = vars(prior), cols = vars(type)) + geom_histogram(aes(y = stat(density)), binwidth = 0.2)




```

```{r}

prior_factor <- rep(as.factor(priors), each=100)

df <- rbind(data.frame(prior=prior_factor, type='x1', value=x1),
            data.frame(prior=prior_factor, type='x2', value=x2),
            data.frame(prior=prior_factor, type='x3', value=x3))

ggplot(df, aes(x=value)) + 
  facet_grid(rows = vars(prior), cols = vars(type)) + geom_histogram(aes(y = stat(density)), binwidth = 0.2)


```
### Question 7H1


```{r}
data(Laffer)
laf_dat <- Laffer %>%
  mutate(tax_rate2 = tax_rate ^ 2,
         across(everything(), standardize))

laf_dat

```
```{r}

laf_line <- brm(tax_revenue ~ 1 + tax_rate, data = laf_dat, family = gaussian,
                prior = c(prior(normal(0, 0.2), class = Intercept),
                          prior(normal(0, 0.5), class = b),
                          prior(exponential(1), class = sigma))
)


laf_poly <- brm(tax_revenue ~ 1 + tax_rate + tax_rate2, data = laf_dat, family = gaussian,
                prior = c(prior(normal(0, 0.2), class = Intercept),
                          prior(normal(0, 0.5), class = b),
                          prior(exponential(1), class = sigma))
)


laf_spln <- brm(tax_revenue ~ 1 + s(tax_rate, bs = "bs"), data = laf_dat,
                family = gaussian,
                prior = c(prior(normal(0, 0.2), class = Intercept),
                          prior(normal(0, 0.5), class = b),
                          prior(normal(0, 0.5), class = sds),
                          prior(exponential(1), class = sigma))
)


```

```{r}
library(tidybayes)


tr_seq <- tibble(tax_rate = seq(0, 40, length.out = 100)) %>%
  mutate(tax_rate2 = tax_rate ^ 2,
         tax_rate = (tax_rate - mean(Laffer$tax_rate)) / sd(Laffer$tax_rate),
         tax_rate2 = (tax_rate2 - mean(Laffer$tax_rate ^ 2)) /
           sd(Laffer$tax_rate ^ 2))

predictions <- bind_rows(
  predicted_draws(laf_line, newdata = tr_seq) %>%
    median_qi(.width = 0.89) %>%
    mutate(type = "Linear"),
  predicted_draws(laf_poly, newdata = tr_seq) %>%
    median_qi(.width = 0.89) %>%
    mutate(type = "Quadratic"),
  predicted_draws(laf_spln, newdata = tr_seq) %>%
    median_qi(.width = 0.89) %>%
    mutate(type = "Spline")
)


fits <- bind_rows(
  fitted_draws(laf_line, newdata = tr_seq) %>%
    median_qi(.width = c(0.67, 0.89, 0.97)) %>%
    mutate(type = "Linear"),
  fitted_draws(laf_poly, newdata = tr_seq) %>%
    median_qi(.width = c(0.67, 0.89, 0.97)) %>%
    mutate(type = "Quadratic"),
  fitted_draws(laf_spln, newdata = tr_seq) %>%
    median_qi(.width = c(0.67, 0.89, 0.97)) %>%
    mutate(type = "Spline")
)

ggplot() +
  facet_wrap(~type, ncol = 2) +
  geom_ribbon(data = predictions,
              aes(x = tax_rate, ymin = .lower, ymax = .upper),
              alpha = 0.2) +
  geom_lineribbon(data = fits,
                  aes(x = tax_rate, y = .value, ymin = .lower, ymax = .upper),
                  size = 0.6) +
  geom_point(data = laf_dat, aes(x = tax_rate, y = tax_revenue),
             alpha = 0.5) +
  scale_fill_brewer(palette = "Blues", breaks = c(0.67, 0.89, 0.97)) +
  labs(x = "Standardized Tax Rate", y = "Standardized Tax Revenue") +
  theme(legend.position = "bottom")

```
```{r}
loo_compare(loo(laf_line), loo(laf_poly), loo(laf_spln))

```

### Question 7H2


```{r}
which.max(loo(laf_line)$pointwise[,'influence_pareto_k'])
```



```{r}
laf_spln2 <- brm(bf(tax_revenue ~ 1 + s(tax_rate, bs = "bs"), nu = 1),
                 data = laf_dat, family = student,
                 prior = c(prior(normal(0, 0.2), class = Intercept),
                           prior(normal(0, 0.5), class = b),
                           prior(normal(0, 0.5), class = sds),
                           prior(exponential(1), class = sigma)))


tr_seq <- tibble(tax_rate = seq(0, 40, length.out = 100)) %>%
  mutate(tax_rate2 = tax_rate ^ 2,
         tax_rate = (tax_rate - mean(Laffer$tax_rate)) / sd(Laffer$tax_rate),
         tax_rate2 = (tax_rate2 - mean(Laffer$tax_rate ^ 2)) /
           sd(Laffer$tax_rate ^ 2))

predictions <- bind_rows(
  predicted_draws(laf_spln2, newdata = tr_seq) %>%
    median_qi(.width = 0.89) %>%
    mutate(type = "Spline")
)


fits <- bind_rows(
  fitted_draws(laf_spln2, newdata = tr_seq) %>%
    median_qi(.width = c(0.67, 0.89, 0.97)) %>%
    mutate(type = "Spline")
)


ggplot() +
  geom_ribbon(data = predictions,
              aes(x = tax_rate, ymin = .lower, ymax = .upper),
              alpha = 0.2) +
  geom_lineribbon(data = fits,
                  aes(x = tax_rate, y = .value, ymin = .lower, ymax = .upper),
                  size = 0.6) +
  geom_point(data = laf_dat, aes(x = tax_rate, y = tax_revenue),
             alpha = 0.5) +
  scale_fill_brewer(palette = "Blues", breaks = c(0.67, 0.89, 0.97)) +
  labs(x = "Standardized Tax Rate", y = "Standardized Tax Revenue") +
  theme(legend.position = "bottom")


```
```{r}
loo_compare(loo(laf_spln), loo(laf_spln2))
```
```{r}
pp_check(laf_spln2) + xlim(c(-10, 10))
```

### Question 7H3

Consider three fictional Polynesian islands. On each there is a Royal Ornithologist charged by
the king with surveying the bird population. They have each found the following proportions of 5
important bird species:
Species A Species B Species C Species D Species E
Island 1 0.2 0.2 0.2 0.2 0.2
Island 2 0.8 0.1 0.05 0.025 0.025
Island 3 0.05 0.15 0.7 0.05 0.05
Notice that each row sums to 1, all the birds. This problem has two parts. It is not computationally
complicated. But it is conceptually tricky. First, compute the entropy of each island’s bird distribution.
Interpret these entropy values. Second, use each island’s bird distribution to predict the other two.
This means to compute the K-L Divergence of each island from the others, treating each island as if
it were a statistical model of the other islands. You should end up with 6 different K-L Divergence
values. Which island predicts the others best? Why?


H(p) = - E log(pi)

```{r}

i1 <- c(0.2, 0.2, 0.2, 0.2, 0.2)
i2 <- c(0.8, 0.1, 0.05, 0.025, 0.025)
i3 <- c(0.05, 0.15, 0.7, 0.05, 0.05)

H1 <- i1 %>% sapply(FUN=function(x) {x * log(x) * -1}) %>% sum
H2 <- i2 %>% sapply(FUN=function(x) {x * log(x) * -1}) %>% sum
H3 <- i3 %>% sapply(FUN=function(x) {x * log(x) * -1}) %>% sum

KL <- function(to_island, from_island) { 
    c(to_island, from_island) %>% 
    matrix(byrow=TRUE, ncol=5) %>% 
    apply(MARGIN=2, FUN=function(x) {x[1]*(log(x[1]) - log(x[2]))}) %>%
    sum
  }


result_matrix <- matrix(
  c(KL(i1,i1), KL(i1,i2), KL(i1,i3),
  KL(i2,i1), KL(i2,i2), KL(i2,i3),
  KL(i3,i1), KL(i3,i2), KL(i3,i3)),
  ncol = 3
)

result_matrix


```

### Question 7H4

7H4. Recall the marriage, age, and happiness collider bias example from Chapter 6. Run models m6.9 and m6.10 again (page 178). Compare these two models using WAIC (or PSIS, they will produce identical results). Which model is expected to make better predictions? Which model provides the correct causal inference about the influence of age on happiness? Can you explain why the answers to these two questions disagree?



```{r, results='hide', include=FALSE}

d <- sim_happiness(seed = 1977, N_years = 1000)

dat <- d %>%
  filter(age > 17) %>%
  mutate(a = (age - 18) / (65 - 18),
         mid = factor(married + 1, labels = c("single", "married")))


m6.9 <- brm(happiness ~ 0 + mid + a, data = dat, family = gaussian,
            prior = c(prior(normal(0, 1), class = b, coef = midmarried),
                      prior(normal(0, 1), class = b, coef = midsingle),
                      prior(normal(0, 2), class = b, coef = a),
                      prior(exponential(1), class = sigma)))

m6.10 <- brm(happiness ~ 1 + a, data = dat, family = gaussian,
            prior = c(prior(normal(0, 1), class = Intercept),
                      prior(normal(0, 2), class = b, coef = a),
                      prior(exponential(1), class = sigma)))



```


```{r}

loo_compare(loo(m6.9), loo(m6.10))

```
### Question 7H5

7H5. Revisit the urban fox data, data(foxes), from the previous chapter’s practice problems. Use
WAIC or PSIS based model comparison on five different models, each using weight as the outcome,
and containing these sets of predictor variables:
(1) avgfood + groupsize + area
(2) avgfood + groupsize
(3) groupsize + area
(4) avgfood
(5) area
Can you explain the relative differences in WAIC scores, using the fox DAG from last week’s homework? Be sure to pay attention to the standard error of the score differences (dSE)


```{r, results='hide', include=FALSE}
data(foxes)

fox_dat <- foxes %>%
  as_tibble() %>%
  select(area, avgfood, weight, groupsize) %>%
  mutate(across(everything(), standardize))

b7h5_2 <- brm(weight ~ 1 + avgfood + groupsize, data = fox_dat,
              family = gaussian,
              prior = c(prior(normal(0, 0.2), class = Intercept),
                        prior(normal(0, 0.5), class = b),
                        prior(exponential(1), class = sigma)))

b7h5_3 <- brm(weight ~ 1 + groupsize + area, data = fox_dat,
              family = gaussian,
              prior = c(prior(normal(0, 0.2), class = Intercept),
                        prior(normal(0, 0.5), class = b),
                        prior(exponential(1), class = sigma)))

b7h5_4 <- brm(weight ~ 1 + avgfood, data = fox_dat,
              family = gaussian,
              prior = c(prior(normal(0, 0.2), class = Intercept),
                        prior(normal(0, 0.5), class = b),
                        prior(exponential(1), class = sigma)))
                        
b7h5_5 <- brm(weight ~ 1 + area, data = fox_dat,
              family = gaussian,
              prior = c(prior(normal(0, 0.2), class = Intercept),
                        prior(normal(0, 0.5), class = b),
                        prior(exponential(1), class = sigma)))

```


```{r}

loo_compare(loo(b7h5_2), loo(b7h5_3), loo(b7h5_4), loo(b7h5_5))

```

